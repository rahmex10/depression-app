{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile DepressionApp.py\n",
    "\n",
    "######################\n",
    "# Import libraries\n",
    "######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######################\n",
    "# Custom function\n",
    "######################\n",
    "## Working on the text processing function\n",
    "\n",
    "data1 = pd.read_csv('clean_d_tweets.csv')\n",
    "data2 = pd.read_csv('clean_non_d_tweets.csv')\n",
    "data1.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest' ], 1, inplace= True)\n",
    "Label = np.random.choice(np.array(['Depressed']),3082 )\n",
    "Label = Label.transpose()\n",
    "Label = pd.DataFrame(Label)\n",
    "Label = Label.rename(columns= {0: 'Label'})\n",
    "data1 = data1.join(Label)\n",
    "data2.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest', 'username' ], 1, inplace= True)\n",
    "Label2 = np.random.choice(np.array(['Not_Depressed']), 4687)\n",
    "Label2 = Label2.transpose()\n",
    "Label2 = pd.DataFrame(Label2)\n",
    "Label2 = Label2.rename(columns= {0: 'Label'})\n",
    "data2 = data2.join(Label2)\n",
    "data1.dropna(inplace= True)\n",
    "data2.dropna(inplace= True)\n",
    "data1.drop(['language', 'username'], 1, inplace= True)\n",
    "data2.drop('language', 1, inplace= True)\n",
    "combined_data = pd.concat([data1, data2], 0, ignore_index= True )\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "\n",
    "pickle.dump(cv, open('CountVectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>depressingmsgs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>cardi b want to trademark her catchphrase okur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>i will bet kellyanne and george conway have pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7766</th>\n",
       "      <td>fan be always ask me how they can watch the ol...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7767</th>\n",
       "      <td>ray romano be a hilarious comedian a kind soul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7768</th>\n",
       "      <td>muellers report may be finish but mine be out ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet        username  \\\n",
       "0     the real reason why you be sad you be attach t...  depressingmsgs   \n",
       "1         my biggest problem be overthinking everything  depressingmsgs   \n",
       "2     the worst sadness be the sadness you have teac...  depressingmsgs   \n",
       "3     i cannot make you understand i cannot make any...  depressingmsgs   \n",
       "4     i do not think anyone really understand how ti...  depressingmsgs   \n",
       "...                                                 ...             ...   \n",
       "7764  cardi b want to trademark her catchphrase okur...             NaN   \n",
       "7765  i will bet kellyanne and george conway have pr...             NaN   \n",
       "7766  fan be always ask me how they can watch the ol...             NaN   \n",
       "7767  ray romano be a hilarious comedian a kind soul...             NaN   \n",
       "7768  muellers report may be finish but mine be out ...             NaN   \n",
       "\n",
       "              Label  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "...             ...  \n",
       "7764  Not_Depressed  \n",
       "7765  Not_Depressed  \n",
       "7766  Not_Depressed  \n",
       "7767  Not_Depressed  \n",
       "7768  Not_Depressed  \n",
       "\n",
       "[7769 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Depressed'], dtype='<U13')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile DepressionApp.py\n",
    "\n",
    "######################\n",
    "# Import libraries\n",
    "######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "######################\n",
    "# Page Title\n",
    "######################\n",
    "\n",
    "image = Image.open('depression.jpg')\n",
    "\n",
    "st.image(image, use_column_width=True)\n",
    "\n",
    "st.write(\"\"\"\n",
    "# Depression Predictor App\n",
    "This app predicts the likelihood of a quotes either **Depressive or not**\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "######################\n",
    "######################\n",
    "\n",
    "st.sidebar.header('User Input Tweets / Quotes')\n",
    "\n",
    "## Read SMILES input\n",
    "tweet_input = \"I am very sad\"\n",
    "\n",
    "tweets = st.sidebar.text_area(\"Tweet input\", tweet_input)\n",
    "tweets = \"C\\n\" + tweets #Adds C as a dummy, first item\n",
    "tweets = tweets.split('\\n')\n",
    "\n",
    "st.header('Input Tweet / Quote' )\n",
    "tweets[1:] # Skips the dummy first item\n",
    "\n",
    "## Calculate depressive descriptors\n",
    "st.header('Predictions')\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "#bagofwords = cv.transform(combined_data['tweet'])\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(tweets[1:])\n",
    "cv = pickle.load(open('CountVectorizer.pkl', 'rb'))\n",
    "bagofwords = cv.transform(pd.Series(tweets[1:]))\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit(bagofwords)\n",
    "tfidff = tfidf.transform(bagofwords)\n",
    "\n",
    "######################\n",
    "# Pre-built model\n",
    "######################\n",
    "\n",
    "# Reads in saved model\n",
    "load_model = pickle.load(open('model.pkl', 'rb'))\n",
    "# Apply model to make predictions\n",
    "prediction = load_model.predict(tfidff)\n",
    "prediction\n",
    "\n",
    "#st.header('Predicted Tweets')\n",
    "#prediction[1:] # Skips the dummy first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DepressionApp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile DepressionApp.py\n",
    "\n",
    "######################\n",
    "# Import libraries\n",
    "######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import nltk\n",
    "#import time\n",
    "#import seaborn as sb\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "######################\n",
    "# Custom function\n",
    "######################\n",
    "## Working on the text processing function\n",
    "\n",
    "data1 = pd.read_csv('clean_d_tweets.csv')\n",
    "data2 = pd.read_csv('clean_non_d_tweets.csv')\n",
    "data1.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest' ], 1, inplace= True)\n",
    "#Label = pd.DataFrame(np.random.choice(np.array(['Depressed']),3082).transpose())\n",
    "#Label = Label.transpose()\n",
    "#Label = pd.DataFrame(Label)\n",
    "#Label = Label.rename(columns= {0: 'Label'})\n",
    "#data1 = data1.join(Label)\n",
    "data2.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest', 'username' ], 1, inplace= True)\n",
    "Label2, Label = pd.DataFrame(np.random.choice(np.array(['Not_Depressed']), 4687).transpose()), pd.DataFrame(np.random.choice(np.array(['Depressed']),3082).transpose())\n",
    "Label, Label2 = Label.rename(columns= {0: 'Label'}), Label2.rename(columns= {0: 'Label'})\n",
    "\n",
    "#Label2 = Label2.transpose()\n",
    "#Label2 = pd.DataFrame(Label2)\n",
    "#Label2 = Label2.rename(columns= {0: 'Label'})\n",
    "data2, data1 = data2.join(Label2), data1.join(Label)\n",
    "data1.dropna(inplace= True)\n",
    "data2.dropna(inplace= True)\n",
    "data1.drop(['language', 'username'], 1, inplace= True)\n",
    "data2.drop('language', 1, inplace= True)\n",
    "combined_data = pd.concat([data1, data2], 0, ignore_index= True )\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "\n",
    "\n",
    "######################\n",
    "# Page Title\n",
    "######################\n",
    "\n",
    "image = Image.open('depression.jpg')\n",
    "\n",
    "st.image(image, use_column_width=True)\n",
    "\n",
    "st.write(\"\"\"\n",
    "# Depression Predictor App\n",
    "This app predicts the likelihood of a quotes to being **Depressive or not**\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "######################\n",
    "# Input molecules (Side Panel)\n",
    "######################\n",
    "\n",
    "st.sidebar.header('User Input Tweets / Quotes')\n",
    "\n",
    "## Read SMILES input\n",
    "tweet_input = \"I am very sad\"\n",
    "\n",
    "tweets = st.sidebar.text_area(\"Tweet input\", tweet_input)\n",
    "tweets = \"C\\n\" + tweets #Adds C as a dummy, first item\n",
    "tweets = tweets.split('\\n')\n",
    "\n",
    "st.header('Input Tweet / Quote' )\n",
    "tweets[1:] # Skips the dummy first item\n",
    "\n",
    "## Calculate depressive descriptors\n",
    "st.header('Predicted Tweets / Quotes')\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "#bagofwords = cv.transform(combined_data['tweet'])\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(tweets[1:])\n",
    "bagofwords = cv.transform(pd.Series(tweets[1:]))\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidff = TfidfTransformer().fit_transform(bagofwords)\n",
    "#X = generate(SMILES)\n",
    "#X[1:] # Skips the dummy first item\n",
    "\n",
    "######################\n",
    "# Pre-built model\n",
    "######################\n",
    "\n",
    "# Reads in saved model\n",
    "load_model = pickle.load(open('model.pkl', 'rb'))\n",
    "# Apply model to make predictions\n",
    "prediction = load_model.predict(tfidff)\n",
    "prediction\n",
    "\n",
    "\n",
    "#st.header('Predicted Tweets')\n",
    "#prediction[1:] # Skips the dummy first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
