{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "<ipython-input-1-ecc0c454ad96>:23: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data1.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
      "<ipython-input-1-ecc0c454ad96>:34: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data2.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
      "<ipython-input-1-ecc0c454ad96>:49: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data1.drop(['language', 'username'], 1, inplace= True)\n",
      "<ipython-input-1-ecc0c454ad96>:50: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data2.drop('language', 1, inplace= True)\n",
      "<ipython-input-1-ecc0c454ad96>:51: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  combined_data = pd.concat([data1, data2], 0, ignore_index= True )\n",
      "2022-09-28 10:28:36.648 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.23.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Depressed'], dtype='<U13')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile DepressionApp.py\n",
    "\n",
    "######################\n",
    "# Import libraries\n",
    "######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import nltk\n",
    "#import time\n",
    "#import seaborn as sb\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "######################\n",
    "# Custom function\n",
    "######################\n",
    "## Working on the text processing function\n",
    "\n",
    "data1 = pd.read_csv('clean_d_tweets.csv')\n",
    "data2 = pd.read_csv('clean_non_d_tweets.csv')\n",
    "data1.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest' ], 1, inplace= True)\n",
    "#Label = pd.DataFrame(np.random.choice(np.array(['Depressed']),3082).transpose())\n",
    "#Label = Label.transpose()\n",
    "#Label = pd.DataFrame(Label)\n",
    "#Label = Label.rename(columns= {0: 'Label'})\n",
    "#data1 = data1.join(Label)\n",
    "data2.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest', 'username' ], 1, inplace= True)\n",
    "Label2, Label = pd.DataFrame(np.random.choice(np.array(['Not_Depressed']), 4687).transpose()), pd.DataFrame(np.random.choice(np.array(['Depressed']),3082).transpose())\n",
    "Label, Label2 = Label.rename(columns= {0: 'Label'}), Label2.rename(columns= {0: 'Label'})\n",
    "\n",
    "#Label2 = Label2.transpose()\n",
    "#Label2 = pd.DataFrame(Label2)\n",
    "#Label2 = Label2.rename(columns= {0: 'Label'})\n",
    "data2, data1 = data2.join(Label2), data1.join(Label)\n",
    "data1.dropna(inplace= True)\n",
    "data2.dropna(inplace= True)\n",
    "data1.drop(['language', 'username'], 1, inplace= True)\n",
    "data2.drop('language', 1, inplace= True)\n",
    "combined_data = pd.concat([data1, data2], 0, ignore_index= True )\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "\n",
    "\n",
    "######################\n",
    "# Page Title\n",
    "######################\n",
    "\n",
    "image = Image.open('depression.jpg')\n",
    "\n",
    "st.image(image, use_column_width=True)\n",
    "\n",
    "st.write(\"\"\"\n",
    "# Depression Predictor App\n",
    "This app predicts the likelihood of a quotes to being **Depressive or not**\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "######################\n",
    "# Input molecules (Side Panel)\n",
    "######################\n",
    "\n",
    "st.sidebar.header('User Input Tweets / Quotes')\n",
    "\n",
    "## Read SMILES input\n",
    "tweet_input = \"I am very sad\"\n",
    "\n",
    "tweets = st.sidebar.text_area(\"Tweet input\", tweet_input)\n",
    "tweets = \"C\\n\" + tweets #Adds C as a dummy, first item\n",
    "tweets = tweets.split('\\n')\n",
    "\n",
    "st.header('Input Tweet / Quote' )\n",
    "tweets[1:] # Skips the dummy first item\n",
    "\n",
    "## Calculate depressive descriptors\n",
    "st.header('Predicted Tweets / Quotes')\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "#bagofwords = cv.transform(combined_data['tweet'])\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(tweets[1:])\n",
    "bagofwords = cv.transform(pd.Series(tweets[1:]))\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidff = TfidfTransformer().fit_transform(bagofwords)\n",
    "#X = generate(SMILES)\n",
    "#X[1:] # Skips the dummy first item\n",
    "\n",
    "######################\n",
    "# Pre-built model\n",
    "######################\n",
    "\n",
    "# Reads in saved model\n",
    "load_model = pickle.load(open('model.pkl', 'rb'))\n",
    "# Apply model to make predictions\n",
    "prediction = load_model.predict(tfidff)\n",
    "prediction\n",
    "\n",
    "\n",
    "#st.header('Predicted Tweets')\n",
    "#prediction[1:] # Skips the dummy first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jinjaNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached Jinja-1.2.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [7 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Abdulrahman\\AppData\\Local\\Temp\\pip-install-p2p9iho9\\jinja_84b402ded3974633a63815911404f724\\setup.py\", line 28\n",
      "      except DistutilsError, e:\n",
      "                           ^\n",
      "  SyntaxError: invalid syntax\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install jinja --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from Pandas) (1.23.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->Pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-924aece2d456>:23: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data1.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
      "<ipython-input-13-924aece2d456>:34: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data2.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
      "<ipython-input-13-924aece2d456>:49: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data1.drop(['language', 'username'], 1, inplace= True)\n",
      "<ipython-input-13-924aece2d456>:50: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data2.drop('language', 1, inplace= True)\n",
      "<ipython-input-13-924aece2d456>:51: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  combined_data = pd.concat([data1, data2], 0, ignore_index= True )\n"
     ]
    }
   ],
   "source": [
    "#%%writefile DepressionApp.py\n",
    "\n",
    "######################\n",
    "# Import libraries\n",
    "######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import nltk\n",
    "#import time\n",
    "#import seaborn as sb\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "######################\n",
    "# Custom function\n",
    "######################\n",
    "## Working on the text processing function\n",
    "\n",
    "data1 = pd.read_csv('clean_d_tweets.csv')\n",
    "data2 = pd.read_csv('clean_non_d_tweets.csv')\n",
    "data1.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest' ], 1, inplace= True)\n",
    "#Label = pd.DataFrame(np.random.choice(np.array(['Depressed']),3082).transpose())\n",
    "#Label = Label.transpose()\n",
    "#Label = pd.DataFrame(Label)\n",
    "#Label = Label.rename(columns= {0: 'Label'})\n",
    "#data1 = data1.join(Label)\n",
    "data2.drop(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'hashtags', 'cashtags', 'user_id', 'user_id_str'\n",
    "           , 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
    "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest', 'username' ], 1, inplace= True)\n",
    "Label2, Label = pd.DataFrame(np.random.choice(np.array(['Not_Depressed']), 4687).transpose()), pd.DataFrame(np.random.choice(np.array(['Depressed']),3082).transpose())\n",
    "Label, Label2 = Label.rename(columns= {0: 'Label'}), Label2.rename(columns= {0: 'Label'})\n",
    "\n",
    "#Label2 = Label2.transpose()\n",
    "#Label2 = pd.DataFrame(Label2)\n",
    "#Label2 = Label2.rename(columns= {0: 'Label'})\n",
    "data2, data1 = data2.join(Label2), data1.join(Label)\n",
    "data1.dropna(inplace= True)\n",
    "data2.dropna(inplace= True)\n",
    "data1.drop(['language', 'username'], 1, inplace= True)\n",
    "data2.drop('language', 1, inplace= True)\n",
    "combined_data = pd.concat([data1, data2], 0, ignore_index= True )\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "pickle.dump(text_process, open('text_process.pkl', 'wb'))\n",
    "pickle.dump(combined_data, open('combined_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.23.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Depressed'], dtype='<U13')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import nltk\n",
    "\n",
    "txtproc = pickle.load(open('text_process.pkl', 'rb'))\n",
    "combined_data = pickle.load(open('combined_data.pkl', 'rb')) \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer= txtproc).fit(combined_data['tweet'])\n",
    "\n",
    "\n",
    "######################\n",
    "# Page Title\n",
    "######################\n",
    "\n",
    "image = Image.open('depression.jpg')\n",
    "\n",
    "st.image(image, use_column_width=True)\n",
    "\n",
    "st.write(\"\"\"\n",
    "# Depression Predictor App\n",
    "This app predicts the likelihood of a quotes to being **Depressive or not**\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "######################\n",
    "# Input molecules (Side Panel)\n",
    "######################\n",
    "\n",
    "st.sidebar.header('User Input Tweets / Quotes')\n",
    "\n",
    "## Read SMILES input\n",
    "tweet_input = \"I am very sad\"\n",
    "\n",
    "tweets = st.sidebar.text_area(\"Tweet input\", tweet_input)\n",
    "tweets = \"C\\n\" + tweets #Adds C as a dummy, first item\n",
    "tweets = tweets.split('\\n')\n",
    "\n",
    "st.header('Input Tweet / Quote' )\n",
    "tweets[1:] # Skips the dummy first item\n",
    "\n",
    "## Calculate depressive descriptors\n",
    "st.header('Predicted Tweets / Quotes')\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(combined_data['tweet'])\n",
    "#bagofwords = cv.transform(combined_data['tweet'])\n",
    "#cv = CountVectorizer(analyzer= text_process).fit(tweets[1:])\n",
    "bagofwords = cv.transform(pd.Series(tweets[1:]))\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidff = TfidfTransformer().fit_transform(bagofwords)\n",
    "#X = generate(SMILES)\n",
    "#X[1:] # Skips the dummy first item\n",
    "\n",
    "######################\n",
    "# Pre-built model\n",
    "######################\n",
    "\n",
    "# Reads in saved model\n",
    "load_model = pickle.load(open('model.pkl', 'rb'))\n",
    "# Apply model to make predictions\n",
    "prediction = load_model.predict(tfidff)\n",
    "prediction\n",
    "\n",
    "\n",
    "#st.header('Predicted Tweets')\n",
    "#prediction[1:] # Skips the dummy first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.text_process(mess)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('combined_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7740</th>\n",
       "      <td>cardi b want to trademark her catchphrase okur...</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7741</th>\n",
       "      <td>i will bet kellyanne and george conway have pr...</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7742</th>\n",
       "      <td>fan be always ask me how they can watch the ol...</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7743</th>\n",
       "      <td>ray romano be a hilarious comedian a kind soul...</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>muellers report may be finish but mine be out ...</td>\n",
       "      <td>Not_Depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7745 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet          Label\n",
       "0     the real reason why you be sad you be attach t...      Depressed\n",
       "1         my biggest problem be overthinking everything      Depressed\n",
       "2     the worst sadness be the sadness you have teac...      Depressed\n",
       "3     i cannot make you understand i cannot make any...      Depressed\n",
       "4     i do not think anyone really understand how ti...      Depressed\n",
       "...                                                 ...            ...\n",
       "7740  cardi b want to trademark her catchphrase okur...  Not_Depressed\n",
       "7741  i will bet kellyanne and george conway have pr...  Not_Depressed\n",
       "7742  fan be always ask me how they can watch the ol...  Not_Depressed\n",
       "7743  ray romano be a hilarious comedian a kind soul...  Not_Depressed\n",
       "7744  muellers report may be finish but mine be out ...  Not_Depressed\n",
       "\n",
       "[7745 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
